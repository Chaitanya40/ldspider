#summary Proposal for a new main page
===Introduction===
The *ldspider* project aims to build a web crawling framework for the linked data web. Requirements and challenges for crawling the linked data web are different from regular web crawling, thus this projects offer a web crawler adapted to traverse and harvest sources and instances from the linked data web. We offer a single jar which can be easily integrated into own applications.

The project is a co-operation between Andreas Harth at [http://www.aifb.kit.edu/ AIFB] and Juergen Umbrich and Aidan Hogan at [http://www.deri.ie/ DERI].

===Features===
 * Can extract RDF statements from different formats:
  * Includes handlers to read RDF/XML, N-TRIPLES and N-QUADS.
  * Can communicate with an Any23 server to extract RDF from a variety of formats such as RDFa and different microformats.
  * Can easily be extended with custom content handlers to handle additional formats.
 * Supports different crawling strategies and can optionally crawl schema information.
 * Crawling can easily be restricted to specific pages e.g. pages with a specific domain prefix.
 * The crawled data can be written in various ways:
  * The output can be written to files in different formats, such as RDF/XML or N-QUADS
  * The crawler can write all statements to a Triple Store using SPARQL/Update. Optionally uses named graphs to structure the written statements by their source page.
 * The output optionally include provenance information.

===Getting Started=== 
*ldSpider* can be used in two ways:
 * Through a command line application. [GettingStartedCommandLine Getting started using ldSpider from the command line]
 * Through a flexible API, which provides various [Hooks] to extend the behaviour of the crawler. [GettingStartedAPI Getting started using the ldSpider API]