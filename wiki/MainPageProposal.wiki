#summary Proposal for a new main page
==Introduction==
The *ldSpider* project aims to build a web crawling framework for the linked data web. Requirements and challenges for crawling the linked data web are different from regular web crawling, thus this projects offer a web crawler adapted to traverse and harvest sources and instances from the linked data web. We offer a single jar which can be easily integrated into own applications.

The project is a co-operation between Andreas Harth at [http://www.aifb.kit.edu/ AIFB]and Juergen Umbrich at [http://www.deri.ie/ DERI]. Aidan Hogan and Robert Isele are contributing.

==Features==
 * *Content Handlers for different formats*:
  * Includes handlers to read RDF/XML, N-TRIPLES and N-QUADS;
  * Handler to communicate with an [http://any23.org/ Any23 server] to extract RDF from a variety of formats such as RDFa and different microformats (for further information see the [http://developers.any23.org/ Any23 developer page];
  * Simple interface design to implement own handlers (e.g. to handle additional formats).
 * *Different crawling strategies* 
  * Breadth-first crawl;
  * Depth-first crawl;
  * optionally crawl schema information (TBox).
 * *Crawling scope* 
  * crawl can easily be restricted to specific pages e.g. pages with a specific domain prefix.
 * *Output formats* - The crawled data can be written in various ways:
  * The output can be written to files in different formats, such as RDF/XML or N-QUADS
  * The crawler can write all statements to a Triple Store using SPARQL/Update. Optionally uses named graphs to structure the written statements by their source page.
  * Optionally, the output include provenance information.

===Getting Started=== 
*ldSpider* can be used in two ways:
 * Through a command line application. [GettingStartedCommandLine Getting started (CLI)]
 * Through a flexible API, which provides various [Hooks] to extend the behavior of the crawler. [GettingStartedAPI Getting started (API)]