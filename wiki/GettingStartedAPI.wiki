#summary Getting started with the Java API of ldSpider

=Introduction=
TODO
=Contents=
<wiki:toc max_depth="3" />
=Prerequisites=
TODO
=Crawler Setup=
==Creating a new Crawler==
{{{
Crawler crawler = new Crawler(numberOfThreads);
}}}
==Setting the Frontier==
{{{
Frontier frontier = new BasicFrontier();
frontier.setBlacklist(CrawlerConstants.BLACKLIST);
frontier.add(new URI(seedUri));
}}}
==Setting the Link Filter==
We can set a link filter to specify which links are retrieved from a parsed document.
In this example, we restrict the crawling to a specific domain using a Link Filter:
{{{
LinkFilter linkFilter = new LinkFilterDomain(frontier);  
linkFilter.addHost(hostUri);  
crawler.setLinkFilter(linkFilter);
}}}
An overview over all available Link Filters can be found [Hooks#LinkFilter here]
==Setting the Content Handler==
By default, ldSpider will handle all documents which use the RDF/XML format. If we want to handler different formats, we can register a Content Handler. Here we combine an RDF/XML Handler and an Nx Handler to handle the most commonly used Linked Data formats:
{{{
ContantHandler h = new ContentHandlers(new ContentHandlerRdfXml(), new ContentHandlerNx());
crawler.setContentHandler(h);
}}}
An overview over all available Content Handlers can be found [Hooks#ContentHandler here]
==Setting the Sink==

==Error handling==

==Start crawling==
We instruct the crawler to start crawling pages by calling an evaluate method.
In this example, we use the breadth first strategy, which limits depth of the traversal (number of rounds), as well as the maximum number of URIs crawled per round.
The breadth first strategy can be configured to crawl the schema information, in which case it will do an extra round to get the schema information of the last round.
{{{
int depth = 2;
int maxURIs = 100;
boolean includeABox = true;
boolean includeTBox = false;

crawler.evaluateBreadthFirst(frontier, depth, maxURIs, includeABox, includeTBox);
}}}