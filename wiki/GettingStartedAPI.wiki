#summary Getting started with the Java API of ldSpider

=Introduction=
TODO
=Contents=
<wiki:toc max_depth="3" />
=Prerequisites=
TODO
=Crawler Setup=
==Creating a new Crawler==
{{{
Crawler crawler = new Crawler(numberOfThreads);
}}}
==Setting the Frontier==
{{{
Frontier frontier = new BasicFrontier();
frontier.setBlacklist(CrawlerConstants.BLACKLIST);
frontier.add(new URI(seedUri));
}}}
==Setting the Link Filter==
We can set a link filter to specify which links are retrieved from a parsed document.
In this example, we restrict the crawling to a specific domain using a Link Filter:
{{{
LinkFilter linkFilter = new LinkFilterDomain(frontier);  
linkFilter.addHost(hostUri);  
crawler.setLinkFilter(linkFilter);
}}}
An overview over all available Link Filters can be found [Hooks#LinkFilter here]
==Setting the Content Handler==
By default, ldSpider will handle all documents which use the RDF/XML format. If we want to handler different formats, we can register a Content Handler. Here we combine an RDF/XML Handler and an Nx Handler to handle the most commonly used Linked Data formats:
{{{
ContentHandler h = new ContentHandlers(new ContentHandlerRdfXml(), new ContentHandlerNx());
crawler.setContentHandler(h);
}}}
An overview over all available Content Handlers can be found [Hooks#ContentHandler here]
==Setting the Sink==
Finally, we need to define a sink, which can be used by the Crawler to write the extracted statements. In this example, we write all statements to a file using the N-QUADS format. For his purpose, we use the SinkCallback class which can use an arbitrary callback from the NxParser library (http://sw.deri.org/2006/08/nxparser/) to write the statements. Among others, NxParser includes a callback to write N-QUADS:
{{{
OutputStream os = new FileOutputStream(outputFile);
Sink sink = new SinkCallback(new CallbackNQOutputStream(os));
crawler.setOutputCallback(sink);
}}}
An overview over all available Sinks can be found [Hooks#Sink here]
==Error handling==

==Start crawling==
We instruct the crawler to start crawling pages by calling an evaluate method.
In this example, we use the breadth first strategy, which limits depth of the traversal (number of rounds), as well as the maximum number of URIs crawled per round.
The breadth first strategy can be configured to crawl the schema information, in which case it will do an extra round to get the schema information of the last round.
{{{
int depth = 2;
int maxURIs = 100;
boolean includeABox = true;
boolean includeTBox = false;

crawler.evaluateBreadthFirst(frontier, depth, maxURIs, includeABox, includeTBox);
}}}